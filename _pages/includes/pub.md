# ğŸ“ Selected Publications

My full paper list can be found at <a href='https://scholar.google.com/citations?user=jTAxkbEAAAAJ'><img src="https://img.shields.io/endpoint?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2Flikyoo%2Flikyoo.github.io@google-scholar-stats%2Fgs_data_shieldsio.json&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>.

## Rmote Sensing Image Segmentation


<div class='paper-box'><div class='paper-box-image'><div><div class="badge" style="font-size: 1.0em;"><b>SegEarth-OV series</b></div><img src='images/segearth_ov_series.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">


[SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for Remote Sensing Images](https://arxiv.org/abs/2410.01768) (<b><font color="red">CVPR'2025 Oral</font></b>) \\
**Kaiyu Li**, Ruixun Liu, Xiangyong Cao, Xueru Bai, Feng Zhou, Deyu Meng, Zhi Wang

[**Project**](https://likyoo.github.io/SegEarth-OV) | [**Code**](https://github.com/likyoo/SegEarth-OV) ![](https://img.shields.io/github/stars/likyoo/SegEarth-OV?style=social) | [**Bilibili**](https://www.bilibili.com/video/BV1pauJzvEo2?spm_id_from=333.1387.collection.video_card.click) | [**Youtube**](https://www.youtube.com/watch?v=xpR7iKW1lSs) | [**Demo**](https://colab.research.google.com/drive/1a-NNz_2maesvszk4Xff5PKY02_moPqt6#scrollTo=Pz9QGEcFBGtK)
  - SegEarth-OV is the first to introduce training-free Open Vocabulary Semantic Segmentation into remote sensing images, which makes OVSS possible in remote sensing contexts.

[Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Images](https://arxiv.org/abs/2508.18067) (arXiv'2025) \\
**Kaiyu Li**, Xiangyong Cao, Ruixun Liu, Shihong Wang, Zixuan Jiang, Zhi Wang, Deyu Meng

[**Code**](https://github.com/earth-insights/SegEarth-OV-2) ![](https://img.shields.io/github/stars/earth-insights/SegEarth-OV-2)
  - We extend [SegEarth-OV](https://likyoo.github.io/SegEarth-OV) to SAR images. This is the first open vocabulary semantic segmentation work for SAR images.

[SegEarth-OV3: Exploring SAM 3 for Open-Vocabulary Semantic Segmentation in Remote Sensing Images](https://arxiv.org/abs/2512.08730) (arXiv'2025) \\
**Kaiyu Li**, Shengqi Zhang, Yupeng Deng, Zhi Wang, Deyu Meng, Xiangyong Cao

[**Code**](https://github.com/earth-insights/SegEarth-OV-3) ![](https://img.shields.io/github/stars/earth-insights/SegEarth-OV-3)
  - We adapte SAM 3 for remote sensing OVSS.

**Media coverage:**<span style="font-size: 12px;"> [ã€é¥æ„Ÿä¸æ·±åº¦å­¦ä¹ ã€‘SegEarth-OV: é¢å‘é¥æ„Ÿå›¾åƒçš„æ— è®­ç»ƒå¼€æ”¾è¯æ±‡åˆ†å‰²](https://mp.weixin.qq.com/s/9QjsMNO4VbF4oc3lKi6IMg)|[ã€GISeré˜¿å…´ã€‘](https://mp.weixin.qq.com/s/sm6hGJCIfKMmnlZEZkuh9A)|[ã€å‡è®ºã€‘](https://mp.weixin.qq.com/s/MnhDCOJP4nUoEg_A8QXuNw)| [ã€é¥æ„Ÿä¸æ·±åº¦å­¦ä¹ ã€‘SegEarth-OV2: è¥¿å®‰äº¤å¤§å›¢é˜Ÿæå‡ºæ— æ ‡æ³¨é¥æ„Ÿå›¾åƒå¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ¡†æ¶](https://mp.weixin.qq.com/s/UH1vVQgceVu_JMR04tAeXA) | [SegEarthOV3: æ¢ç´¢SAM 3åœ¨é¥æ„Ÿå›¾åƒå¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ä¸­çš„åº”ç”¨](https://mp.weixin.qq.com/s/UpBSKHe5IGPGi_4dlDblUg) | [2025å¹´50å‘¨æœ€é«˜äººæ°”è®ºæ–‡ï¼šè¥¿äº¤æ›¹ç›¸æ¹§ã€å­Ÿå¾·å®‡å›¢é˜ŸSegEarth-OV3ï¼](https://mp.weixin.qq.com/s/iOudJCET9EaK_ygbpJwzVQ) </span>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge" style="font-size: 1.0em;"><b>SegEarth-R series</b></div><img src='images/segearth_r_series.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SegEarth-R1: Geospatial Pixel Reasoning via Large Language Model](https://arxiv.org/abs/2504.09644) (arXiv'2025) \\
**Kaiyu Li**, Zepeng Xin, Li Pang, Chao Pang, Yupeng Deng, Jing Yao, Guisong Xia, Deyu Meng, Zhi Wang, Xiangyong Cao

[**Project**](https://earth-insights.github.io/SegEarth-R1) | [**Dataset**](https://huggingface.co/datasets/earth-insights/EarthReason) | [**Code**](https://github.com/earth-insights/SegEarth-R1) ![](https://img.shields.io/github/stars/earth-insights/SegEarth-R1?style=social)
  - We introduce the geospatial pixel reasoning task, construct the first benchmark dataset (EarthReason), and propose a simple yet effective baseline (SegEarth-R1).

[SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images](https://arxiv.org/abs/2512.20013) (arXiv'2025) \\
Zepeng Xin\*, **Kaiyu Li\***, Luodi Chen, Wanchen Li, Yuchen Xiao, Hui Qiao, Weizhan Zhang, Deyu Meng, Xiangyong Cao. (Project leader)

[**Code**](https://github.com/earth-insights/SegEarth-R2) ![](https://img.shields.io/github/stars/earth-insights/SegEarth-R2?style=social)
  - We propose a comprehensive remote sensing language-guided segmentation benchmark dataset (LaSeRS), and improve our method to the more powerful SegEarth-R2.

**Media coverage:**<span style="font-size: 12px;"> [ã€é¥æ„Ÿä¸æ·±åº¦å­¦ä¹ ã€‘SegEarth-R1: é¥æ„Ÿ+æ¨ç†å¤§æ¨¡å‹ï¼Geospatial Pixel Reasoning](https://mp.weixin.qq.com/s/qu_Agarhj8nqcjWv7ZA7YQ)|[ã€è§†è§‰ä¸é¥æ„Ÿå‰æ²¿ã€‘SegEarth-R1å®ç°åœ°ç†æ¨ç†ã€Œç±»äººæ€è€ƒã€](https://mp.weixin.qq.com/s/9XL49Sa7nj1KMTjbRXFrXw)|[ã€å‡è®ºã€‘](https://mp.weixin.qq.com/s/eVDl4im-_op8svPGtd8jnA)|</span>

</div>
</div>

- **[Semi-supervised Segmentation]** [RS-MTDF: Multi-Teacher Distillation and Fusion for Remote Sensing Semi-Supervised Semantic Segmentation](https://arxiv.org/abs/2506.08772), Jiayi Song\*, **Kaiyu Li\***, Xiangyong Cao, Deyu Meng. arXiv 2025. [Code](https://github.com/earth-insights/RS-MTDF). (Project leader)

- **[Few-shot Segmentation]** [Class Similarity Transition: Decoupling Class Similarities and Imbalance from Generalized Few-shot Segmentation](https://arxiv.org/abs/2404.05111), Shihong Wang\*, Ruixun Liu\*, **Kaiyu Li\***, Jiawei Jiang, Xiangyong Cao. CVPRW'2024. [Code](https://github.com/earth-insights/ClassTrans). ğŸ†ï¸The 2nd place in the [CVPR 2024 OpenEarthMap Land Cover Mapping Few-Shot Challenge](https://cliffbb.github.io/OEM-Fewshot-Challenge/). (Project leader)

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPRW 2024</div><img src='images/classtrans.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Class Similarity Transition: Decoupling Class Similarities and Imbalance from Generalized Few-shot Segmentation](https://arxiv.org/abs/2404.05111) \\
Shihong Wang\*, Ruixun Liu\*, **Kaiyu Li\***, Jiawei Jiang, Xiangyong Cao

[**Code**](https://github.com/earth-insights/ClassTrans) ![](https://img.shields.io/github/stars/earth-insights/ClassTrans?style=social) <strong><span class='show_paper_citations' data='jTAxkbEAAAAJ:eQOLeE2rZwMC'></span></strong>
  - We propose a similarity transition matrix to guide the learning of novel classes with base class knowledge.
  - Our solution wins 2nd place in the [CVPR 2024 OpenEarthMap Land Cover Mapping Few-Shot Challenge](https://cliffbb.github.io/OEM-Fewshot-Challenge/).
</div>
</div> -->


## Remote Sensing Image Change Detection

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2026 Oral</div><img src='images/dynamicearth.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[DynamicEarth: How Far are We from Open-Vocabulary Change Detection?](https://arxiv.org/abs/2501.12931) \\
**Kaiyu Li**, Xiangyong Cao, Yupeng Deng, Chao Pang, Zepeng Xin, Deyu Meng, Zhi Wang

[**Project**](https://likyoo.github.io/DynamicEarth) ![](https://img.shields.io/github/stars/likyoo/DynamicEarth?style=social)
  - We propose a new task, open-vocabulary change detection (OVCD).
  - We propose two universal frameworks for OVCD.
  - The first OVCD codebase, DynamicEarth, is released.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM 2025</div><img src='images/opencd.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Open-CD: A Comprehensive Toolbox for Change Detection](https://arxiv.org/abs/2407.15317) \\
**Kaiyu Li**, Jiawei Jiang, Andrea Codegoni, Chengxi Han, Yupeng Deng, Keyan Chen, Zhuo Zheng, Hao Chen, Zhengxia Zou, Zhenwei Shi, Sheng Fang, Deyu Meng, Zhi Wang, Xiangyong Cao

[**Project**](https://github.com/likyoo/open-cd) ![](https://img.shields.io/github/stars/likyoo/open-cd?style=social)
  - Open-CD is one of the most popular change detection toolkits.
  - We launch the Open-CD Technical Report Plan (Open-CD TRP for shot). We invite some authors to introduce their algorithms and participate in the construction of the Open-CD codebase. This plan is under active development and we will keep this report updated.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TGRS 2024</div><img src='images/semicd_vl.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[SemiCD-VL: Visual-Language Model Guidance Makes Better Semi-supervised Change Detector](https://arxiv.org/abs/2405.04788) \\
**Kaiyu Li**, Xiangyong Cao, Yupeng Deng, Junmin Liu, Deyu Meng, Zhi Wang

[**Code**](https://github.com/likyoo/SemiCD-VL) ![](https://img.shields.io/github/stars/likyoo/SemiCD-VL?style=social)
  - This work is the first to introduce visual language models to the semi-supervised change detection task. 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TGRS 2024</div><img src='images/ban.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[A New Learning Paradigm for Foundation Model-based Remote Sensing Change Detection](https://arxiv.org/abs/2312.01163) \\
**Kaiyu Li**, Xiangyong Cao, Deyu Meng

ğŸ†ï¸ <b><font color="red">ESI Highly Cited Paper</font></b>

[**Code**](https://github.com/likyoo/BAN) ![](https://img.shields.io/github/stars/likyoo/BAN?style=social) <strong><span class='show_paper_citations' data='jTAxkbEAAAAJ:YsMSGLbcyi4C'></span></strong>

  - BAN is the first universal framework to adapt the foundation model to the change detection task.
</div>
</div>

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">TGRS 2023</div><img src='images/changer.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Changer: Feature interaction is what you need for change detection](https://arxiv.org/abs/2209.08290) \\
Sheng Fang, **Kaiyu Liâ€ **, Zhe Li

ğŸ†ï¸ <b><font color="red">ESI Highly Cited Paper</font></b>

[**Code**](https://github.com/likyoo/open-cd) ![](https://img.shields.io/github/stars/likyoo/open-cd?style=social) <strong><span class='show_paper_citations' data='jTAxkbEAAAAJ:IjCSPb-OGe4C'></span></strong>
  - We propose a novel general change detection architecture, MetaChanger, which includes a series of alternative interaction layers in the feature extractor.
  - To verify the effectiveness of MetaChanger, we propose two derived models, ChangerAD and ChangerEx with simple interaction strategies: Aggregation-Distribution (AD) and â€œexchangeâ€. AD is abstracted from some complex interaction methods, and **â€œexchangeâ€ is a completely parameter & computation-free operation by exchanging bi-temporal features**.
</div>
</div> -->

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">GRSL 2021</div><img src='images/snunet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SNUNet-CD: A densely connected Siamese network for change detection of VHR images](https://ieeexplore.ieee.org/abstract/document/9355573) \\
Sheng Fang, **Kaiyu Liâ€ **, Jinyuan Shao, Zhe Li

ğŸ†ï¸ <b><font color="red">ESI Highly Cited Paper</font></b>

[**Code**](https://github.com/likyoo/Siam-NestedUNet) ![](https://img.shields.io/github/stars/likyoo/Siam-NestedUNet?style=social) <strong><span class='show_paper_citations' data='jTAxkbEAAAAJ:9yKSN-GCB0IC'></span></strong>

- We propose a densely connected siamese network for change detection, namely SNUNet-CD (the combination of Siamese network and NestedUNet). SNUNet-CD alleviates the loss of localization information in the deep layers of neural network through compact information transmission between encoder and decoder, and between decoder and decoder.
- Ensemble Channel Attention Module (ECAM) is  proposed for deep supervision.
</div>
</div> -->

- **[Supervised Change Detection]** [Changer: Feature interaction is what you need for change detection](https://arxiv.org/abs/2209.08290), Sheng Fang, **Kaiyu Liâ€ **, Zhe Li. TGRS 2023. ğŸ†ï¸ <b><font color="red">ESI Highly Cited Paper</font></b>. [Code](https://github.com/likyoo/open-cd) ![](https://img.shields.io/github/stars/likyoo/open-cd?style=social) <strong><span class='show_paper_citations' data='jTAxkbEAAAAJ:IjCSPb-OGe4C'></span></strong>.
- **[Supervised Change Detection]** [SNUNet-CD: A densely connected Siamese network for change detection of VHR images](https://ieeexplore.ieee.org/abstract/document/9355573), Sheng Fang, **Kaiyu Liâ€ **, Jinyuan Shao, Zhe Li. GRSL 2021. ğŸ†ï¸ <b><font color="red">ESI Highly Cited Paper</font></b>. [Code](https://github.com/likyoo/Siam-NestedUNet) ![](https://img.shields.io/github/stars/likyoo/Siam-NestedUNet?style=social) <strong><span class='show_paper_citations' data='jTAxkbEAAAAJ:9yKSN-GCB0IC'></span></strong>.
- **[Change Captioning]** [MV-CC: Mask Enhanced Video Model for Remote Sensing Change Captioning](https://arxiv.org/abs/2410.23946), Ruixun Liu\*, **Kaiyu Li\***, Jiayi Song\*, Dongwei Sun, Xiangyong Cao. arXiv 2024. [Code](https://github.com/liuruixun/MV-cc). (Project leader)

<!-- ## Multi-modal Remote Sensing
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">GRSL 2022</div><img src='images/s2enet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SÂ²ENet: Spatialâ€“Spectral Cross-Modal Enhancement Network for Classification of Hyperspectral and LiDAR Data](https://ieeexplore.ieee.org/abstract/document/9583936) \\
Sheng Fang, **Kaiyu Liâ€ **, Zhe Li

ğŸ†ï¸ <b><font color="red">ESI Highly Cited Paper</font></b>

[**Code**](https://github.com/likyoo/Multimodal-Remote-Sensing-Toolkit) ![](https://img.shields.io/github/stars/likyoo/Multimodal-Remote-Sensing-Toolkit?style=social) <strong><span class='show_paper_citations' data='jTAxkbEAAAAJ:d1gkVwhDpl0C'></span></strong>

  - We propose a Spatial-Spectral Enhancement Module (SÂ²EM) for cross modal information interaction in deep neural networks. Specifically, SÂ²EM consists of SpAtial Enhancement Module (SAEM) for enhancing spatial representation of hyperspectral data by LiDAR features and SpEctral Enhancement Module (SEEM) for enhancing spectral representation of LiDAR data by hyperspectral features.
</div>
</div> -->


## Others

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2025</div><img src='images/EarthAgent.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Designing Domain-Specific Agents via Hierarchical Task Abstraction Mechanism](https://arxiv.org/abs/2511.17198) \\
**Kaiyu Li\***, Jiayu Wang*, Zhi Wang, Hui Qiao, Weizhan Zhang, Deyu Meng, Xiangyong Cao

[**Project**](https://earth-insights.github.io/EarthAgent) |  [**GeoPlan-bench**](https://github.com/earth-insights/GeoPlan-bench) ![](https://img.shields.io/github/stars/earth-insights/GeoPlan-bench?style=social) | [**EarthAgent Code**](https://github.com/earth-insights/EarthAgent) ![](https://img.shields.io/github/stars/earth-insights/EarthAgent?style=social)
  - We propose HTAM for designing domain-specific agents, instantiate this framework in the remote sensing field as EarthAgent, and introduce a comprehensive evaluation platform, GeoPlan-bench, to assess complex remote sensing planning capabilities.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2025</div><img src='images/describeearth.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[DescribeEarth: Describe Anything for Remote Sensing Images](https://arxiv.org/abs/2509.25654) \\
**Kaiyu Li\***, Zixuan Jiang*, Xiangyong Cao, Jiayu Wang, Yuchen Xiao, Deyu Meng, Zhi Wang

[**Dataset**](https://huggingface.co/datasets/earth-insights/DE-Dataset) |  [**Benchmark**](https://huggingface.co/datasets/earth-insights/DE-Benchmark) | [**Code**](https://github.com/earth-insights/DescribeEarth) ![](https://img.shields.io/github/stars/earth-insights/DescribeEarth?style=social)
  - We introduce geo-spatial detailed localized captioning.
  - We build the first describe anything model in remote sensing.
  - We release the related dataset and benchmark.

**Media coverage:**<span style="font-size: 12px;"> [ã€é¥æ„Ÿä¸æ·±åº¦å­¦ä¹ ã€‘DescribeEarth: è¥¿äº¤å›¢é˜Ÿæå‡ºé¥æ„Ÿæè¿°ä¸€åˆ‡æ¨¡å‹, é¼ æ ‡è½»ç‚¹å³å¾—ç›®æ ‡è¯¦ç»†æè¿°.](https://mp.weixin.qq.com/s/qhFIZ6QMmikZ9L7q3cKFaw)|[ã€ç ç§‘æ™ºèƒ½ã€‘ä¸‡ç‰©çš†å¯æè¿°ï¼é¥æ„Ÿå›¾åƒæè¿°è¿›å…¥å¯¹è±¡çº§ç»†ç²’åº¦ç†è§£æ—¶ä»£](https://mp.weixin.qq.com/s/FjmlKo0EkEzXhAk82AdeFQ)|[ã€CVç‚¼ä¸¹æœ¯ã€‘DescribeEarthï¼šè¥¿å®‰äº¤å¤§æœ€æ–°è®ºæ–‡â€”â€”é¢å‘é¥æ„Ÿå›¾åƒçš„ç»†ç²’åº¦å¯¹è±¡æè¿°æ–°èŒƒå¼ï¼](https://mp.weixin.qq.com/s/LfGuwxEoIwpEOZXAi6w0fg)|</span>

</div>
</div>


- **[Remote Sening MLLM]** [ZoomEarth: Active Perception for Ultra-High-Resolution Geospatial Vision-Language Tasks](https://arxiv.org/abs/2511.12267), Ruixun Liu*, Bowen Fu*, Jiayi Song, **Kaiyu Li**, Wanchen Li, Lanxuan Xue, Hui Qiao, Weizhan Zhang, Deyu Meng, Xiangyong Cao. arXiv'2025. [Project](https://earth-insights.github.io/ZoomEarth/). (Project leader)
- **[Remote Sensing Vector Map Dataset]** [IRSAMAP: A Large-Scale, High-Resolution, Multi-Class Vector Map Dataset for Remote Sensing](https://ieeexplore.ieee.org/document/11129926), Yu Meng, Ligao Deng, Zhihao Xi, Jiansheng Chen, Jingbo Chen, Anzhi Yue, Diyou Liu, Kai Li, Chenhao Wang, **Kaiyu Li**, Yupeng Deng, Xian Sun. TGRS'2025. [Dataset](https://github.com/ucas-dlg/IRSAMap).
- **[UAV Tracking]** [Dist-Tracker: A Small Object-aware Detector and Tracker for UAV Tracking](https://openaccess.thecvf.com/content/CVPR2025W/Anti-UAV/html/Wang_Dist-Tracker_A_Small_Object-aware_Detector_and_Tracker_for_UAV_Tracking_CVPRW_2025_paper.html), Wenzhen Wang\*, Jing Fu\*, Jiayi Song\*, **Kaiyu Li\***, Hui Qiao, Jiang Liu, Hao Sun, Xiangyong Cao. CVPRW 2025. [Code](https://github.com/earth-insights/Dist-Tracker). (Project leader)
- **[Zero-shot Hyperspectral Image Classification]** [SPECIAL: Zero-shot Hyperspectral Image Classification With CLIP](https://arxiv.org/abs/2501.16222), Li Pang, Jing Yao, **Kaiyu Li**, Xiangyong Cao. arXiv 2025. [Code](https://github.com/LiPang/SPECIAL).
- **[Road Graph Extraction]** [Towards Satellite Image Road Graph Extraction: A Global-Scale Dataset and A Novel Method](https://arxiv.org/abs/2411.16733), Pan Yin\*, **Kaiyu Li\***, Xiangyong Cao, Jing Yao, Lei Liu, Xueru Bai, Feng Zhou, Deyu Meng. CVPR 2025. [Code](https://github.com/earth-insights/samroadplus). (Project leader)
<!-- - **[Change Captioning]** [MV-CC: Mask Enhanced Video Model for Remote Sensing Change Captioning](https://arxiv.org/abs/2410.23946), Ruixun Liu\*, **Kaiyu Li\***, Jiayi Song\*, Dongwei Sun, Xiangyong Cao. arXiv 2024. [Code](https://github.com/liuruixun/MV-cc). -->
- **[Agriculture]** [RepDI: A light-weight CPU network for apple leaf disease identification](https://www.sciencedirect.com/science/article/abs/pii/S0168169923005100), Jiye Zheng, **Kaiyu Liâ€ **, Wenbin Wu, Huaijun Ruan. COMPAG 2023. [Code](https://github.com/likyoo/RepDI).
- **[Skeleton Extraction]** [CAMION: Cascade Multi-input Multi-output Network for Skeleton Extraction](https://openaccess.thecvf.com/content/CVPR2022W/DLGC/html/Fang_CAMION_Cascade_Multi-Input_Multi-Output_Network_for_Skeleton_Extraction_CVPRW_2022_paper.html), Sheng Fang, **Kaiyu Liâ€ **, Zhe Li. CVPRW 2022.
- **[Multi-modal Remote Sensing]** [SÂ²ENet: Spatialâ€“Spectral Cross-Modal Enhancement Network for Classification of Hyperspectral and LiDAR Data](https://ieeexplore.ieee.org/abstract/document/9583936), Sheng Fang, **Kaiyu Liâ€ **, Zhe Li. GRSL 2022. [Code](https://github.com/likyoo/Multimodal-Remote-Sensing-Toolkit). ğŸ†ï¸ <b><font color="red">ESI Highly Cited Paper</font></b>



<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPRW 2022</div><img src='images/camion.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[CAMION: Cascade Multi-input Multi-output Network for Skeleton Extraction](https://openaccess.thecvf.com/content/CVPR2022W/DLGC/html/Fang_CAMION_Cascade_Multi-Input_Multi-Output_Network_for_Skeleton_Extraction_CVPRW_2022_paper.html) \\
Sheng Fang, **Kaiyu Liâ€ **, Zhe Li

  - We propose a general cascade deep learning pipeline that achieves competitive skeleton extraction performance only using a simple U-shape network.
  - We propose CAMION, a CAscade Multi-Input multiOutput Network that can obtain better performance from several auxiliary tasks such as feature point detection and contour extraction.
  - Our solution wins 5th place in the CVPR 2022 DLGC Pixel SkelNetOn Challenge.
</div>
</div> -->