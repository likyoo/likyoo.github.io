# ğŸ“ Selected Publications

My full paper list can be found at <a href='https://scholar.google.com/citations?user=jTAxkbEAAAAJ'><img src="https://img.shields.io/endpoint?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2Flikyoo%2Flikyoo.github.io@google-scholar-stats%2Fgs_data_shieldsio.json&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>.

## Open-Vocabulary Segmentation

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2024</div><img src='images/segearth_ov.png' alt="sym" width="90%"></div></div>
<div class='paper-box-text' markdown="1">

[SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for Remote Sensing Images](https://arxiv.org/abs/2410.01768) \\
**Kaiyu Li**, Ruixun Liu, Xiangyong Cao, Xueru Bai, Feng Zhou, Deyu Meng, Zhi Wang

[**Project**](https://likyoo.github.io/SegEarth-OV) | [**Code**](https://github.com/likyoo/SegEarth-OV) ![](https://img.shields.io/github/stars/likyoo/SegEarth-OV?style=social) | [**Demo**](https://colab.research.google.com/drive/1a-NNz_2maesvszk4Xff5PKY02_moPqt6#scrollTo=Pz9QGEcFBGtK)
  - SegEarth-OV is the first to introduce training-free Open Vocabulary Semantic Segmentation into remote sensing images, which makes OVSS possible in remote sensing contexts.

<span style="font-size: 12px;">[å…¬ä¼—å·ã€é¥æ„Ÿä¸æ·±åº¦å­¦ä¹ ã€‘ï¼šè®ºæ–‡ | SegEarth-OV: é¢å‘é¥æ„Ÿå›¾åƒçš„æ— è®­ç»ƒå¼€æ”¾è¯æ±‡åˆ†å‰²](https://mp.weixin.qq.com/s/9QjsMNO4VbF4oc3lKi6IMg)</span>

<span style="font-size: 12px;">[å…¬ä¼—å·ã€GISeré˜¿å…´ã€‘ï¼šé¥æ„Ÿè®ºæ–‡ | Arxiv | SegEarth-OVï¼šä¸€ç§åŸºäºCLIPå’ŒFeatUpçš„é¥æ„Ÿå›¾åƒå¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ–¹æ³•ï¼Œä»£ç å·²å¼€æºï¼](https://mp.weixin.qq.com/s/sm6hGJCIfKMmnlZEZkuh9A)</span>

</div>
</div>

## Remote Sensing Image Change Detection

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2024</div><img src='images/opencd.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Open-CD: A Comprehensive Toolbox for Change Detection](https://arxiv.org/abs/2407.15317) \\
**Kaiyu Li**, Jiawei Jiang, Andrea Codegoni, Chengxi Han, Yupeng Deng, Keyan Chen, Zhuo Zheng, Hao Chen, Zhengxia Zou, Zhenwei Shi, Sheng Fang, Deyu Meng, Zhi Wang, Xiangyong Cao

[**Project**](https://github.com/likyoo/open-cd) ![](https://img.shields.io/github/stars/likyoo/open-cd?style=social)
  - Open-CD is one of the most popular change detection toolkits.
  - We launch the Open-CD Technical Report Plan (Open-CD TRP for shot). We invite some authors to introduce their algorithms and participate in the construction of the Open-CD codebase. This plan is under active development and we will keep this report updated.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2024</div><img src='images/semicd_vl.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[SemiCD-VL: Visual-Language Model Guidance Makes Better Semi-supervised Change Detector](https://arxiv.org/abs/2405.04788) \\
**Kaiyu Li**, Xiangyong Cao, Yupeng Deng, Junmin Liu, Deyu Meng, Zhi Wang

[**Code**](https://github.com/likyoo/SemiCD-VL) ![](https://img.shields.io/github/stars/likyoo/SemiCD-VL?style=social)
  - This work is the first to introduce visual language models to a semi-supervised change detection task. 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TGRS 2024</div><img src='images/ban.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[A New Learning Paradigm for Foundation Model-based Remote Sensing Change Detection](https://arxiv.org/abs/2312.01163) \\
**Kaiyu Li**, Xiangyong Cao, Deyu Meng

ğŸ†ï¸ <b><font color="red">ESI Highly Cited Paper</font></b>

[**Code**](https://github.com/likyoo/BAN) ![](https://img.shields.io/github/stars/likyoo/BAN?style=social) <strong><span class='show_paper_citations' data='jTAxkbEAAAAJ:YsMSGLbcyi4C'></span></strong>

  - BAN is the first universal framework to adapt the foundation model to the change detection task.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TGRS 2023</div><img src='images/changer.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Changer: Feature interaction is what you need for change detection](https://arxiv.org/abs/2209.08290) \\
Sheng Fang, **Kaiyu Liâ€ **, Zhe Li

ğŸ†ï¸ <b><font color="red">ESI Highly Cited Paper</font></b>

[**Code**](https://github.com/likyoo/open-cd) ![](https://img.shields.io/github/stars/likyoo/open-cd?style=social) <strong><span class='show_paper_citations' data='jTAxkbEAAAAJ:IjCSPb-OGe4C'></span></strong>
  - We propose a novel general change detection architecture, MetaChanger, which includes a series of alternative interaction layers in the feature extractor.
  - To verify the effectiveness of MetaChanger, we propose two derived models, ChangerAD and ChangerEx with simple interaction strategies: Aggregation-Distribution (AD) and â€œexchangeâ€. AD is abstracted from some complex interaction methods, and **â€œexchangeâ€ is a completely parameter&computation-free operation by exchanging bi-temporal features**.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">GRSL 2021</div><img src='images/snunet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SNUNet-CD: A densely connected Siamese network for change detection of VHR images](https://ieeexplore.ieee.org/abstract/document/9355573) \\
Sheng Fang, **Kaiyu Liâ€ **, Jinyuan Shao, Zhe Li

ğŸ†ï¸ <b><font color="red">ESI Highly Cited Paper</font></b>

[**Code**](https://github.com/likyoo/Siam-NestedUNet) ![](https://img.shields.io/github/stars/likyoo/Siam-NestedUNet?style=social) <strong><span class='show_paper_citations' data='jTAxkbEAAAAJ:9yKSN-GCB0IC'></span></strong>

- We propose a densely connected siamese network for change detection, namely SNUNet-CD (the combination of Siamese network and NestedUNet). SNUNet-CD alleviates the loss of localization information in the deep layers of neural network through compact information transmission between encoder and decoder, and between decoder and decoder.
- Ensemble Channel Attention Module (ECAM) is  proposed for deep supervision.
</div>
</div>

## Few-shot Segmentation

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPRW 2024</div><img src='images/classtrans.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Class Similarity Transition: Decoupling Class Similarities and Imbalance from Generalized Few-shot Segmentation](https://arxiv.org/abs/2404.05111) \\
Shihong Wang\*, Ruixun Liu\*, **Kaiyu Li\***, Jiawei Jiang, Xiangyong Cao

[**Code**](https://github.com/earth-insights/ClassTrans) ![](https://img.shields.io/github/stars/earth-insights/ClassTrans?style=social) <strong><span class='show_paper_citations' data='jTAxkbEAAAAJ:eQOLeE2rZwMC'></span></strong>
  - We propose a similarity transition matrix to guide the learning of novel classes with base class knowledge.
  - Our solution wins 2nd place in the [CVPR 2024 OpenEarthMap Land Cover Mapping Few-Shot Challenge](https://cliffbb.github.io/OEM-Fewshot-Challenge/).
</div>
</div>

## Multi-modal Remote Sensing
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">GRSL 2022</div><img src='images/s2enet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SÂ²ENet: Spatialâ€“Spectral Cross-Modal Enhancement Network for Classification of Hyperspectral and LiDAR Data](https://ieeexplore.ieee.org/abstract/document/9583936) \\
Sheng Fang, **Kaiyu Liâ€ **, Zhe Li

ğŸ†ï¸ <b><font color="red">ESI Highly Cited Paper</font></b>

[**Code**](https://github.com/likyoo/Multimodal-Remote-Sensing-Toolkit) ![](https://img.shields.io/github/stars/likyoo/Multimodal-Remote-Sensing-Toolkit?style=social) <strong><span class='show_paper_citations' data='jTAxkbEAAAAJ:d1gkVwhDpl0C'></span></strong>

  - We propose a Spatial-Spectral Enhancement Module (SÂ²EM) for cross modal information interaction in deep neural networks. Specifically, SÂ²EM consists of SpAtial Enhancement Module (SAEM) for enhancing spatial representation of hyperspectral data by LiDAR features and SpEctral Enhancement Module (SEEM) for enhancing spectral representation of LiDAR data by hyperspectral features.
</div>
</div>


## Others
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPRW 2022</div><img src='images/camion.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[CAMION: Cascade Multi-input Multi-output Network for Skeleton Extraction](https://openaccess.thecvf.com/content/CVPR2022W/DLGC/html/Fang_CAMION_Cascade_Multi-Input_Multi-Output_Network_for_Skeleton_Extraction_CVPRW_2022_paper.html) \\
Sheng Fang, **Kaiyu Liâ€ **, Zhe Li

  - We propose a general cascade deep learning pipeline that achieves competitive skeleton extraction performance only using a simple U-shape network.
  - We propose CAMION, a CAscade Multi-Input multiOutput Network that can obtain better performance from several auxiliary tasks such as feature point detection and contour extraction.
  - Our solution wins 5th place in the CVPR 2022 DLGC Pixel SkelNetOn Challenge.
</div>
</div>