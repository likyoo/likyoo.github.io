# üìù Selected Publications

My full paper list can be found at <a href='https://scholar.google.com/citations?user=jTAxkbEAAAAJ'><img src="https://img.shields.io/endpoint?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2Flikyoo%2Flikyoo.github.io@google-scholar-stats%2Fgs_data_shieldsio.json&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>.

## Open-Vocabulary Segmentation

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025</div><img src='images/segearth_ov.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">


[SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for Remote Sensing Images](https://arxiv.org/abs/2410.01768) \\
**Kaiyu Li**, Ruixun Liu, Xiangyong Cao, Xueru Bai, Feng Zhou, Deyu Meng, Zhi Wang

[**Project**](https://likyoo.github.io/SegEarth-OV) | [**Code**](https://github.com/likyoo/SegEarth-OV) ![](https://img.shields.io/github/stars/likyoo/SegEarth-OV?style=social) | [**Demo**](https://colab.research.google.com/drive/1a-NNz_2maesvszk4Xff5PKY02_moPqt6#scrollTo=Pz9QGEcFBGtK)
  - SegEarth-OV is the first to introduce training-free Open Vocabulary Semantic Segmentation into remote sensing images, which makes OVSS possible in remote sensing contexts.

**Media coverage:**<span style="font-size: 12px;"> [„ÄêÈÅ•ÊÑü‰∏éÊ∑±Â∫¶Â≠¶‰π†„ÄëSegEarth-OV: Èù¢ÂêëÈÅ•ÊÑüÂõæÂÉèÁöÑÊó†ËÆ≠ÁªÉÂºÄÊîæËØçÊ±áÂàÜÂâ≤](https://mp.weixin.qq.com/s/9QjsMNO4VbF4oc3lKi6IMg)|[„ÄêGISerÈòøÂÖ¥„Äë](https://mp.weixin.qq.com/s/sm6hGJCIfKMmnlZEZkuh9A)|[„ÄêÂáèËÆ∫„Äë](https://mp.weixin.qq.com/s/MnhDCOJP4nUoEg_A8QXuNw)|</span>

</div>
</div>

## Remote Sensing Image Change Detection

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2025</div><img src='images/dynamicearth.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[DynamicEarth: How Far are We from Open-Vocabulary Change Detection?](https://arxiv.org/abs/2501.12931) \\
**Kaiyu Li**, Xiangyong Cao, Yupeng Deng, Chao Pang, Zepeng Xin, Deyu Meng, Zhi Wang

[**Project**](https://likyoo.github.io/DynamicEarth) ![](https://img.shields.io/github/stars/likyoo/DynamicEarth?style=social)
  - We propose a new task, open-vocabulary change detection (OVCD).
  - We propose two universal frameworks for OVCD.
  - The first OVCD codebase, DynamicEarth, is released.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2024</div><img src='images/opencd.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Open-CD: A Comprehensive Toolbox for Change Detection](https://arxiv.org/abs/2407.15317) \\
**Kaiyu Li**, Jiawei Jiang, Andrea Codegoni, Chengxi Han, Yupeng Deng, Keyan Chen, Zhuo Zheng, Hao Chen, Zhengxia Zou, Zhenwei Shi, Sheng Fang, Deyu Meng, Zhi Wang, Xiangyong Cao

[**Project**](https://github.com/likyoo/open-cd) ![](https://img.shields.io/github/stars/likyoo/open-cd?style=social)
  - Open-CD is one of the most popular change detection toolkits.
  - We launch the Open-CD Technical Report Plan (Open-CD TRP for shot). We invite some authors to introduce their algorithms and participate in the construction of the Open-CD codebase. This plan is under active development and we will keep this report updated.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TGRS 2024</div><img src='images/semicd_vl.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[SemiCD-VL: Visual-Language Model Guidance Makes Better Semi-supervised Change Detector](https://arxiv.org/abs/2405.04788) \\
**Kaiyu Li**, Xiangyong Cao, Yupeng Deng, Junmin Liu, Deyu Meng, Zhi Wang

[**Code**](https://github.com/likyoo/SemiCD-VL) ![](https://img.shields.io/github/stars/likyoo/SemiCD-VL?style=social)
  - This work is the first to introduce visual language models to the semi-supervised change detection task. 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TGRS 2024</div><img src='images/ban.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[A New Learning Paradigm for Foundation Model-based Remote Sensing Change Detection](https://arxiv.org/abs/2312.01163) \\
**Kaiyu Li**, Xiangyong Cao, Deyu Meng

üèÜÔ∏è <b><font color="red">ESI Highly Cited Paper</font></b>

[**Code**](https://github.com/likyoo/BAN) ![](https://img.shields.io/github/stars/likyoo/BAN?style=social) <strong><span class='show_paper_citations' data='jTAxkbEAAAAJ:YsMSGLbcyi4C'></span></strong>

  - BAN is the first universal framework to adapt the foundation model to the change detection task.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TGRS 2023</div><img src='images/changer.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Changer: Feature interaction is what you need for change detection](https://arxiv.org/abs/2209.08290) \\
Sheng Fang, **Kaiyu Li‚Ä†**, Zhe Li

üèÜÔ∏è <b><font color="red">ESI Highly Cited Paper</font></b>

[**Code**](https://github.com/likyoo/open-cd) ![](https://img.shields.io/github/stars/likyoo/open-cd?style=social) <strong><span class='show_paper_citations' data='jTAxkbEAAAAJ:IjCSPb-OGe4C'></span></strong>
  - We propose a novel general change detection architecture, MetaChanger, which includes a series of alternative interaction layers in the feature extractor.
  - To verify the effectiveness of MetaChanger, we propose two derived models, ChangerAD and ChangerEx with simple interaction strategies: Aggregation-Distribution (AD) and ‚Äúexchange‚Äù. AD is abstracted from some complex interaction methods, and **‚Äúexchange‚Äù is a completely parameter & computation-free operation by exchanging bi-temporal features**.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">GRSL 2021</div><img src='images/snunet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SNUNet-CD: A densely connected Siamese network for change detection of VHR images](https://ieeexplore.ieee.org/abstract/document/9355573) \\
Sheng Fang, **Kaiyu Li‚Ä†**, Jinyuan Shao, Zhe Li

üèÜÔ∏è <b><font color="red">ESI Highly Cited Paper</font></b>

[**Code**](https://github.com/likyoo/Siam-NestedUNet) ![](https://img.shields.io/github/stars/likyoo/Siam-NestedUNet?style=social) <strong><span class='show_paper_citations' data='jTAxkbEAAAAJ:9yKSN-GCB0IC'></span></strong>

- We propose a densely connected siamese network for change detection, namely SNUNet-CD (the combination of Siamese network and NestedUNet). SNUNet-CD alleviates the loss of localization information in the deep layers of neural network through compact information transmission between encoder and decoder, and between decoder and decoder.
- Ensemble Channel Attention Module (ECAM) is  proposed for deep supervision.
</div>
</div>

## Few-shot Segmentation

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPRW 2024</div><img src='images/classtrans.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Class Similarity Transition: Decoupling Class Similarities and Imbalance from Generalized Few-shot Segmentation](https://arxiv.org/abs/2404.05111) \\
Shihong Wang\*, Ruixun Liu\*, **Kaiyu Li\***, Jiawei Jiang, Xiangyong Cao

[**Code**](https://github.com/earth-insights/ClassTrans) ![](https://img.shields.io/github/stars/earth-insights/ClassTrans?style=social) <strong><span class='show_paper_citations' data='jTAxkbEAAAAJ:eQOLeE2rZwMC'></span></strong>
  - We propose a similarity transition matrix to guide the learning of novel classes with base class knowledge.
  - Our solution wins 2nd place in the [CVPR 2024 OpenEarthMap Land Cover Mapping Few-Shot Challenge](https://cliffbb.github.io/OEM-Fewshot-Challenge/).
</div>
</div>

## Multi-modal Remote Sensing
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">GRSL 2022</div><img src='images/s2enet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[S¬≤ENet: Spatial‚ÄìSpectral Cross-Modal Enhancement Network for Classification of Hyperspectral and LiDAR Data](https://ieeexplore.ieee.org/abstract/document/9583936) \\
Sheng Fang, **Kaiyu Li‚Ä†**, Zhe Li

üèÜÔ∏è <b><font color="red">ESI Highly Cited Paper</font></b>

[**Code**](https://github.com/likyoo/Multimodal-Remote-Sensing-Toolkit) ![](https://img.shields.io/github/stars/likyoo/Multimodal-Remote-Sensing-Toolkit?style=social) <strong><span class='show_paper_citations' data='jTAxkbEAAAAJ:d1gkVwhDpl0C'></span></strong>

  - We propose a Spatial-Spectral Enhancement Module (S¬≤EM) for cross modal information interaction in deep neural networks. Specifically, S¬≤EM consists of SpAtial Enhancement Module (SAEM) for enhancing spatial representation of hyperspectral data by LiDAR features and SpEctral Enhancement Module (SEEM) for enhancing spectral representation of LiDAR data by hyperspectral features.
</div>
</div>


## Others

- **[Zero-shot Hyperspectral Image Classification]** [SPECIAL: Zero-shot Hyperspectral Image Classification With CLIP](https://arxiv.org/abs/2501.16222), Li Pang, Jing Yao, **Kaiyu Li**, Xiangyong Cao. arXiv 2025.
- **[Road Graph Extraction]** [Towards Satellite Image Road Graph Extraction: A Global-Scale Dataset and A Novel Method](https://arxiv.org/abs/2411.16733), Pan Yin\*, **Kaiyu Li\***, Xiangyong Cao, Jing Yao, Lei Liu, Xueru Bai, Feng Zhou, Deyu Meng. CVPR 2025.
- **[Change Captioning]** [MV-CC: Mask Enhanced Video Model for Remote Sensing Change Captioning](https://arxiv.org/abs/2410.23946), Ruixun Liu\*, **Kaiyu Li\***, Jiayi Song\*, Dongwei Sun, Xiangyong Cao. arXiv 2024.
- **[Agriculture]** [RepDI: A light-weight CPU network for apple leaf disease identification](https://www.sciencedirect.com/science/article/abs/pii/S0168169923005100), Jiye Zheng, **Kaiyu Li‚Ä†**, Wenbin Wu, Huaijun Ruan. COMPAG 2023.
- **[Skeleton Extraction]** [CAMION: Cascade Multi-input Multi-output Network for Skeleton Extraction](https://openaccess.thecvf.com/content/CVPR2022W/DLGC/html/Fang_CAMION_Cascade_Multi-Input_Multi-Output_Network_for_Skeleton_Extraction_CVPRW_2022_paper.html), Sheng Fang, **Kaiyu Li‚Ä†**, Zhe Li. CVPRW 2022.


<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPRW 2022</div><img src='images/camion.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[CAMION: Cascade Multi-input Multi-output Network for Skeleton Extraction](https://openaccess.thecvf.com/content/CVPR2022W/DLGC/html/Fang_CAMION_Cascade_Multi-Input_Multi-Output_Network_for_Skeleton_Extraction_CVPRW_2022_paper.html) \\
Sheng Fang, **Kaiyu Li‚Ä†**, Zhe Li

  - We propose a general cascade deep learning pipeline that achieves competitive skeleton extraction performance only using a simple U-shape network.
  - We propose CAMION, a CAscade Multi-Input multiOutput Network that can obtain better performance from several auxiliary tasks such as feature point detection and contour extraction.
  - Our solution wins 5th place in the CVPR 2022 DLGC Pixel SkelNetOn Challenge.
</div>
</div> -->